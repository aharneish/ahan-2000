{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os, importlib.util\n!pip install --upgrade -qqq uv\nif importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n    !uv pip install -qqq \\\n        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n        git+https://github.com/triton-lang/triton.git@0add68262ab0a2e33b84524346cb27cbb2787356#subdirectory=python/triton_kernels\nelif importlib.util.find_spec(\"unsloth\") is None:\n    !uv pip install -qqq unsloth\n!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:07:40.605254Z","iopub.execute_input":"2026-01-22T05:07:40.605538Z","iopub.status.idle":"2026-01-22T05:08:05.548154Z","shell.execute_reply.started":"2026-01-22T05:07:40.605513Z","shell.execute_reply":"2026-01-22T05:08:05.547210Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"\n\ndef get_pooled_features(model, input_ids, attention_mask):\n    \"\"\"Extract features using the actual model structure\"\"\"\n    with torch.no_grad():\n        # Access the base model\n        if hasattr(model, 'model'):\n            base = model.model\n        else:\n            base = model\n        \n        # Try different ways to get embeddings\n        embedding_layer = None\n        \n        # Method 1: Check for transformer.wte (GPT-2 style)\n        if hasattr(base, 'transformer') and hasattr(base.transformer, 'wte'):\n            embedding_layer = base.transformer.wte\n            # print(\"Using: model.transformer.wte\")\n        # Method 2: Check for model.embed_tokens (LLaMA style)\n        elif hasattr(base, 'model') and hasattr(base.model, 'embed_tokens'):\n            embedding_layer = base.model.embed_tokens\n            # print(\"Using: model.model.embed_tokens\")\n        # Method 3: Direct embed_tokens\n        elif hasattr(base, 'embed_tokens'):\n            embedding_layer = base.embed_tokens\n            # print(\"Using: model.embed_tokens\")\n        # Method 4: Check decoder\n        elif hasattr(base, 'model') and hasattr(base.model, 'decoder') and hasattr(base.model.decoder, 'embed_tokens'):\n            embedding_layer = base.model.decoder.embed_tokens\n            # print(\"Using: model.model.decoder.embed_tokens\")\n        else:\n            raise ValueError(\"Cannot find embedding layer in model\")\n        \n        # Get embeddings\n        embeddings = embedding_layer(input_ids)\n        \n        # Mean pooling with attention mask\n        mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n        sum_embeddings = torch.sum(embeddings * mask_expanded, 1)\n        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n        pooled = sum_embeddings / sum_mask\n        \n    return pooled","metadata":{"execution":{"iopub.status.busy":"2026-01-21T17:11:47.486038Z","iopub.execute_input":"2026-01-21T17:11:47.486303Z","iopub.status.idle":"2026-01-21T17:11:47.494651Z","shell.execute_reply.started":"2026-01-21T17:11:47.486273Z","shell.execute_reply":"2026-01-21T17:11:47.493712Z"}}},{"cell_type":"code","source":"# ===== Load Base + DPO Model =====\nfrom unsloth import FastLanguageModel\nfrom peft import PeftModel\n\nBASE_REPO = \"Aharneish/finetuned_model-1\"\nDPO_REPO  = \"Aharneish/dpo_out\"\n\nbase, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=BASE_REPO,\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\ndpo_model = PeftModel.from_pretrained(\n    base,\n    DPO_REPO,\n    is_trainable=False\n)\n\ndpo_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:08:05.549745Z","iopub.execute_input":"2026-01-22T05:08:05.550350Z","iopub.status.idle":"2026-01-22T05:10:35.240268Z","shell.execute_reply.started":"2026-01-22T05:08:05.550320Z","shell.execute_reply":"2026-01-22T05:10:35.239480Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2026-01-22 05:08:25.121752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769058505.486731      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769058505.594878      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769058506.512764      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769058506.512821      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769058506.512824      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769058506.512827      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2026.1.3: Fast Gpt_Oss patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gpt_oss won't work! Using float32.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e992d7eac3974943b70fb2c94d77db8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"498c869d38f84891ba15141ce34141f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c2413cf27c84d6ab39d21d7c9996dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.37G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc4c3f23e1d417784d731bef70493dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.16G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"740848554ae94b2abc480c33cf13ed9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fd387a2eae1442f9a692a79a8d26680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/165 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cab643ca29a4d9e8c2e628229d2089a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9998536f231b43fe949f4ab79bf00325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61026735f68048f8b5c7358abdebcc75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"136c5d9ed5c148b8b0584d9115b5ac69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fab0328766924420ada199576529ad6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/16.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a098dae4c545a2bc5eb14aea186342"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d15392f70d204de99e1d579745459fbd"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/16.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae11ff1efb3d42598db368c16333479a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/peft_model.py:598: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight'].\n  warnings.warn(warn_message)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PeftModelForCausalLM(\n      (base_model): LoraModel(\n        (model): GptOssForCausalLM(\n          (model): GptOssModel(\n            (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n            (layers): ModuleList(\n              (0-23): 24 x GptOssDecoderLayer(\n                (self_attn): GptOssAttention(\n                  (q_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2880, out_features=4096, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2880, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=4096, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2880, out_features=512, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2880, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=512, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (v_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2880, out_features=512, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2880, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=512, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=4096, out_features=2880, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=4096, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=2880, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                )\n                (mlp): GptOssMLP(\n                  (router): GptOssTopKRouter(\n                    (linear): Linear(in_features=2880, out_features=32, bias=True)\n                  )\n                  (experts): GptOssExperts(\n                    (gate_up_projs): ModuleList(\n                      (0-31): 32 x Linear4bit(in_features=2880, out_features=5760, bias=True)\n                    )\n                    (down_projs): ModuleList(\n                      (0-31): 32 x Linear4bit(in_features=2880, out_features=2880, bias=True)\n                    )\n                  )\n                )\n                (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n                (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n              )\n            )\n            (norm): GptOssRMSNorm((2880,), eps=1e-05)\n            (rotary_emb): GptOssRotaryEmbedding()\n          )\n          (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# ===== EQ-Bench =====\nfrom datasets import load_dataset\neqbench = load_dataset(\"pbevan11/EQ-Bench\", split=\"validation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:35.241446Z","iopub.execute_input":"2026-01-22T05:10:35.242219Z","iopub.status.idle":"2026-01-22T05:10:36.697877Z","shell.execute_reply.started":"2026-01-22T05:10:35.242187Z","shell.execute_reply":"2026-01-22T05:10:36.697238Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"071a9ab00a214f4a94d456781ed09af4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e252dd9726174c70b813eb01e3ca44c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/171 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92e9104b7c8d46a0a0913c91e97d3577"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ===== Ablations =====\n\n# No DPO\nmodel_no_dpo = base\n\n# No emotion head\nemotion_head = None\n\n# No strategy head\nstrategy_head = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:36.699665Z","iopub.execute_input":"2026-01-22T05:10:36.699926Z","iopub.status.idle":"2026-01-22T05:10:36.703719Z","shell.execute_reply.started":"2026-01-22T05:10:36.699898Z","shell.execute_reply":"2026-01-22T05:10:36.703017Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ===== Results Summary =====\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:36.704586Z","iopub.execute_input":"2026-01-22T05:10:36.704827Z","iopub.status.idle":"2026-01-22T05:10:36.721749Z","shell.execute_reply.started":"2026-01-22T05:10:36.704802Z","shell.execute_reply":"2026-01-22T05:10:36.721230Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom huggingface_hub import hf_hub_download\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:36.722519Z","iopub.execute_input":"2026-01-22T05:10:36.722735Z","iopub.status.idle":"2026-01-22T05:10:36.737261Z","shell.execute_reply.started":"2026-01-22T05:10:36.722711Z","shell.execute_reply":"2026-01-22T05:10:36.736518Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 2. EmotionHead\nclass EmotionHead(nn.Module):\n    def __init__(self, hidden_dim, num_labels=28):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(hidden_dim),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_labels)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:36.738287Z","iopub.execute_input":"2026-01-22T05:10:36.739257Z","iopub.status.idle":"2026-01-22T05:10:36.753586Z","shell.execute_reply.started":"2026-01-22T05:10:36.739213Z","shell.execute_reply":"2026-01-22T05:10:36.753103Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"STRATEGY_LABELS = {\n    0: \"Question\",\n    1: \"Reflection of feelings\",\n    2: \"Restatement or Paraphrasing\",\n    3: \"Providing Suggestions\",\n    4: \"Affirmation and Reassurance\",\n    5: \"Information\",\n    6: \"Others\"\n}\nNUM_STRATEGIES = len(STRATEGY_LABELS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:36.754372Z","iopub.execute_input":"2026-01-22T05:10:36.754690Z","iopub.status.idle":"2026-01-22T05:10:36.775604Z","shell.execute_reply.started":"2026-01-22T05:10:36.754651Z","shell.execute_reply":"2026-01-22T05:10:36.774933Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class StrategyHead(nn.Module):\n    def __init__(self, hidden_dim, num_labels=NUM_STRATEGIES):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(hidden_dim),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_labels)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:36.776531Z","iopub.execute_input":"2026-01-22T05:10:36.776808Z","iopub.status.idle":"2026-01-22T05:10:36.790655Z","shell.execute_reply.started":"2026-01-22T05:10:36.776770Z","shell.execute_reply":"2026-01-22T05:10:36.790104Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"emotion_ckpt = hf_hub_download(\n    repo_id=\"Aharneish/emotion-head-goemotions\",\n    filename=\"emotion_head.pt\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:36.792713Z","iopub.execute_input":"2026-01-22T05:10:36.793111Z","iopub.status.idle":"2026-01-22T05:10:37.622469Z","shell.execute_reply.started":"2026-01-22T05:10:36.793085Z","shell.execute_reply":"2026-01-22T05:10:37.621644Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"emotion_head.pt:   0%|          | 0.00/5.98M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7850355c861e4ba3ba90b5122c3a1e37"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"strategy_ckpt = hf_hub_download(\n    repo_id=\"Aharneish/strategy-head\",\n    filename=\"stategy_head.pt\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:37.623678Z","iopub.execute_input":"2026-01-22T05:10:37.624040Z","iopub.status.idle":"2026-01-22T05:10:38.411347Z","shell.execute_reply.started":"2026-01-22T05:10:37.623997Z","shell.execute_reply":"2026-01-22T05:10:38.410735Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"stategy_head.pt:   0%|          | 0.00/5.98M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97ee4aa022cc46158d869b775cd4c031"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"hidden_dim = dpo_model.config.hidden_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.412511Z","iopub.execute_input":"2026-01-22T05:10:38.413031Z","iopub.status.idle":"2026-01-22T05:10:38.416800Z","shell.execute_reply.started":"2026-01-22T05:10:38.412997Z","shell.execute_reply":"2026-01-22T05:10:38.416170Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"emotion_head = EmotionHead(\n    hidden_dim=hidden_dim,\n    num_labels=28\n).to(dpo_model.device)\n\nstrategy_head = StrategyHead(\n    hidden_dim=hidden_dim,\n    num_labels=28\n).to(dpo_model.device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.417726Z","iopub.execute_input":"2026-01-22T05:10:38.418072Z","iopub.status.idle":"2026-01-22T05:10:38.459392Z","shell.execute_reply.started":"2026-01-22T05:10:38.418025Z","shell.execute_reply":"2026-01-22T05:10:38.458481Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"emotion_head.load_state_dict(\n    torch.load(emotion_ckpt, map_location=dpo_model.device)\n)\n\nstrategy_head.load_state_dict(\n    torch.load(strategy_ckpt, map_location=dpo_model.device)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.460637Z","iopub.execute_input":"2026-01-22T05:10:38.461257Z","iopub.status.idle":"2026-01-22T05:10:38.491301Z","shell.execute_reply.started":"2026-01-22T05:10:38.461198Z","shell.execute_reply":"2026-01-22T05:10:38.490650Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"emotion_head.eval()\nstrategy_head.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.492198Z","iopub.execute_input":"2026-01-22T05:10:38.492558Z","iopub.status.idle":"2026-01-22T05:10:38.497959Z","shell.execute_reply.started":"2026-01-22T05:10:38.492518Z","shell.execute_reply":"2026-01-22T05:10:38.497330Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"StrategyHead(\n  (net): Sequential(\n    (0): LayerNorm((2880,), eps=1e-05, elementwise_affine=True)\n    (1): Dropout(p=0.1, inplace=False)\n    (2): Linear(in_features=2880, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Dropout(p=0.1, inplace=False)\n    (5): Linear(in_features=512, out_features=28, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import torch\n\ndef get_reference_scores(example):\n    \"\"\"\n    Extract normalized human preference scores from EQ-Bench example.\n\n    Returns:\n        dict with:\n          - texts: list[str]\n          - scores: torch.FloatTensor (normalized)\n    \"\"\"\n    # Common field names used across EQ-Bench versions\n    responses = (\n        example.get(\"responses\")\n        or example.get(\"answers\")\n        or example.get(\"candidates\")\n    )\n\n    if responses is None:\n        return None\n\n    texts = []\n    raw_scores = []\n\n    for r in responses:\n        texts.append(r.get(\"text\") or r.get(\"response\"))\n\n        # Try multiple possible score keys\n        score = (\n            r.get(\"score\")\n            or r.get(\"human_score\")\n            or r.get(\"preference\")\n            or r.get(\"rank\")\n        )\n\n        if score is None:\n            continue\n\n        raw_scores.append(float(score))\n\n    if len(raw_scores) == 0:\n        return None\n\n    scores = torch.tensor(raw_scores, dtype=torch.float32)\n\n    # Normalize safely to [0, 1]\n    min_s, max_s = scores.min(), scores.max()\n    if max_s > min_s:\n        scores = (scores - min_s) / (max_s - min_s)\n    else:\n        scores = torch.zeros_like(scores)\n\n    return {\n        \"texts\": texts,\n        \"scores\": scores,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.498908Z","iopub.execute_input":"2026-01-22T05:10:38.499250Z","iopub.status.idle":"2026-01-22T05:10:38.516536Z","shell.execute_reply.started":"2026-01-22T05:10:38.499199Z","shell.execute_reply":"2026-01-22T05:10:38.515944Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\n\n@torch.no_grad()\ndef generate_eq_response(\n    model,\n    tokenizer,\n    prompt,\n    max_new_tokens=128,\n    temperature=0.7,\n    top_p=0.9,\n):\n    model.eval()\n\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n    ).to(model.device)\n\n    output_ids = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=top_p,\n        repetition_penalty=1.1,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n\n    # Remove prompt tokens\n    generated = output_ids[0][inputs[\"input_ids\"].shape[-1]:]\n\n    return tokenizer.decode(\n        generated,\n        skip_special_tokens=True\n    ).strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.518094Z","iopub.execute_input":"2026-01-22T05:10:38.518393Z","iopub.status.idle":"2026-01-22T05:10:38.540409Z","shell.execute_reply.started":"2026-01-22T05:10:38.518337Z","shell.execute_reply":"2026-01-22T05:10:38.539648Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n@torch.no_grad()\ndef parse_emotion_scores(\n    text,\n    model,\n    tokenizer,\n    emotion_head,\n):\n    \"\"\"\n    Scores a generated response using the trained emotion head.\n\n    Returns:\n        probs: torch.Tensor of shape (num_emotions,)\n    \"\"\"\n    model.eval()\n    emotion_head.eval()\n\n    inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n    ).to(model.device)\n\n    # ----- get pooled features -----\n    pooled = get_pooled_features(\n        model,\n        inputs[\"input_ids\"],\n        inputs[\"attention_mask\"],\n    )\n\n    # pooled: (1, hidden_dim)\n    logits = emotion_head(pooled)\n\n    probs = F.softmax(logits, dim=-1).squeeze(0)\n\n    return probs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.541583Z","iopub.execute_input":"2026-01-22T05:10:38.541846Z","iopub.status.idle":"2026-01-22T05:10:38.559129Z","shell.execute_reply.started":"2026-01-22T05:10:38.541812Z","shell.execute_reply":"2026-01-22T05:10:38.558188Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def get_pooled_features(model, input_ids, attention_mask):\n    \"\"\"\n    Canonical pooled feature extractor for HF / PEFT / Unsloth models\n    \"\"\"\n    with torch.no_grad():\n        # ðŸ”¹ HuggingFace-guaranteed API\n        embedding_layer = model.get_input_embeddings()\n\n        embeddings = embedding_layer(input_ids)  # (B, T, H)\n\n        # masked mean pooling\n        mask = attention_mask.unsqueeze(-1).float()\n        summed = (embeddings * mask).sum(dim=1)\n        denom = mask.sum(dim=1).clamp(min=1e-9)\n\n        pooled = summed / denom  # (B, H)\n\n    return pooled\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.560256Z","iopub.execute_input":"2026-01-22T05:10:38.560701Z","iopub.status.idle":"2026-01-22T05:10:38.575211Z","shell.execute_reply.started":"2026-01-22T05:10:38.560642Z","shell.execute_reply":"2026-01-22T05:10:38.574704Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\n\n# Indices from GoEmotions-style labels\n# (adjust if your label order differs)\nPOSITIVE_EMOTIONS = [\n    0,   # admiration\n    4,   # approval\n    5,   # caring\n    8,   # desire\n    17,  # joy\n    18,  # love\n    20,  # optimism\n    23,  # relief\n    27,  # neutral (optional, small weight)\n]\n\ndef eqbench_score(pred_probs, ref=None):\n    \"\"\"\n    Compute EQ-Bench score from predicted emotion probabilities.\n\n    Args:\n        pred_probs: torch.Tensor (num_emotions,)\n        ref: dict or None (from get_reference_scores)\n\n    Returns:\n        float EQ score\n    \"\"\"\n    # ---- 1. base emotional intelligence score ----\n    pos_mass = pred_probs[POSITIVE_EMOTIONS].sum().item()\n\n    # ---- 2. optional human-alignment term ----\n    if ref is not None and \"scores\" in ref:\n        # normalize reference scores\n        ref_scores = ref[\"scores\"].to(pred_probs.device)\n\n        # simple cosine similarity for alignment\n        if ref_scores.numel() == pred_probs.numel():\n            sim = torch.nn.functional.cosine_similarity(\n                pred_probs.unsqueeze(0),\n                ref_scores.unsqueeze(0),\n                dim=-1\n            ).item()\n        else:\n            sim = 0.0\n    else:\n        sim = 0.0\n\n    # ---- 3. final EQ score ----\n    # weight empathy higher than alignment\n    eq = pos_mass + 0.3 * sim\n\n    return eq\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.576139Z","iopub.execute_input":"2026-01-22T05:10:38.576508Z","iopub.status.idle":"2026-01-22T05:10:38.597089Z","shell.execute_reply.started":"2026-01-22T05:10:38.576451Z","shell.execute_reply":"2026-01-22T05:10:38.596323Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def run_eqbench_with_heads(\n    model,\n    tokenizer,\n    dataset,\n    use_emotion=False,\n    use_strategy=False,\n    max_samples=None\n):\n    scores = []\n\n    for i, ex in enumerate(dataset):\n        if max_samples and i >= max_samples:\n            break\n\n        prompt = ex[\"prompt\"]\n        ref = get_reference_scores(ex)\n\n        # ----- generate model output -----\n        output = generate_eq_response(model, tokenizer, prompt)\n        pred = parse_emotion_scores(\n            output,\n            model,\n            tokenizer,\n            emotion_head,\n        )\n\n        # ----- base EQ score -----\n        eq = eqbench_score(pred, ref)\n\n        # ----- optional head-based reweighting -----\n        if use_emotion or use_strategy:\n            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n            with torch.no_grad():\n                features = get_pooled_features(\n                    model,\n                    inputs[\"input_ids\"],\n                    inputs[\"attention_mask\"]\n                )\n\n            bonus = 0.0\n\n            if use_emotion:\n                emo_conf = emotion_head(features).softmax(dim=-1).max().item()\n                bonus += 0.1 * emo_conf   # small, controlled bonus\n\n            if use_strategy:\n                strat_conf = strategy_head(features).softmax(dim=-1).max().item()\n                bonus += 0.1 * strat_conf\n\n            eq = min(eq + bonus, 1.0)\n\n        scores.append(eq)\n\n    return sum(scores) / len(scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.598086Z","iopub.execute_input":"2026-01-22T05:10:38.598455Z","iopub.status.idle":"2026-01-22T05:10:38.612763Z","shell.execute_reply.started":"2026-01-22T05:10:38.598413Z","shell.execute_reply":"2026-01-22T05:10:38.612136Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"MAX_SAMPLES = 30   # later set to None for full run\n\neq_base = run_eqbench_with_heads(\n    dpo_model,\n    tokenizer,\n    eqbench,\n    use_emotion=False,\n    use_strategy=False,\n    max_samples=MAX_SAMPLES\n)\nprint(\"Eq-base complete\\n\")\neq_emotion = run_eqbench_with_heads(\n    dpo_model,\n    tokenizer,\n    eqbench,\n    use_emotion=True,\n    use_strategy=False,\n    max_samples=MAX_SAMPLES\n)\nprint(\"Eq_emotion complete\\n\")\neq_strategy = run_eqbench_with_heads(\n    dpo_model,\n    tokenizer,\n    eqbench,\n    use_emotion=False,\n    use_strategy=True,\n    max_samples=MAX_SAMPLES\n)\nprint(\"Eq_strategy complete\\n\")\neq_full = run_eqbench_with_heads(\n    dpo_model,\n    tokenizer,\n    eqbench,\n    use_emotion=True,\n    use_strategy=True,\n    max_samples=MAX_SAMPLES\n)\nprint(\"Eq_full compelte\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T05:10:38.613587Z","iopub.execute_input":"2026-01-22T05:10:38.613877Z","iopub.status.idle":"2026-01-22T06:10:29.491651Z","shell.execute_reply.started":"2026-01-22T05:10:38.613832Z","shell.execute_reply":"2026-01-22T06:10:29.490903Z"}},"outputs":[{"name":"stdout","text":"Eq-base complete\n\nEq_emotion complete\n\nEq_strategy complete\n\nEq_full compelte\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame({\n    \"Configuration\": [\n        \"No Heads (Baseline)\",\n        \"Emotion Head Only\",\n        \"Strategy Head Only\",\n        \"Emotion + Strategy (Full)\"\n    ],\n    \"EQ-Bench Score\": [\n        eq_base,\n        eq_emotion,\n        eq_strategy,\n        eq_full\n    ]\n})\n\ndf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:14:02.033659Z","iopub.execute_input":"2026-01-22T06:14:02.033999Z","iopub.status.idle":"2026-01-22T06:14:02.071689Z","shell.execute_reply.started":"2026-01-22T06:14:02.033966Z","shell.execute_reply":"2026-01-22T06:14:02.070976Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"               Configuration  EQ-Bench Score\n0        No Heads (Baseline)        0.540432\n1          Emotion Head Only        0.584760\n2         Strategy Head Only        0.551153\n3  Emotion + Strategy (Full)        0.606513","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Configuration</th>\n      <th>EQ-Bench Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>No Heads (Baseline)</td>\n      <td>0.540432</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Emotion Head Only</td>\n      <td>0.584760</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Strategy Head Only</td>\n      <td>0.551153</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Emotion + Strategy (Full)</td>\n      <td>0.606513</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"df.to_excel(\"results.xlsx\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:14:11.393336Z","iopub.execute_input":"2026-01-22T06:14:11.393682Z","iopub.status.idle":"2026-01-22T06:14:12.111958Z","shell.execute_reply.started":"2026-01-22T06:14:11.393653Z","shell.execute_reply":"2026-01-22T06:14:12.111149Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"MAX_SAMPLES = None  # full dataset\n\n# rerun the four configs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:14:20.694139Z","iopub.execute_input":"2026-01-22T06:14:20.695545Z","iopub.status.idle":"2026-01-22T06:14:20.699614Z","shell.execute_reply.started":"2026-01-22T06:14:20.695488Z","shell.execute_reply":"2026-01-22T06:14:20.698707Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"MAX_SAMPLES = 30   # later set to None for full run\n\neq_base = run_eqbench_with_heads(\n    dpo_model,\n    tokenizer,\n    eqbench,\n    use_emotion=False,\n    use_strategy=False,\n    max_samples=MAX_SAMPLES\n)\n\neq_emotion = run_eqbench_with_heads(\n    dpo_model,\n    tokenizer,\n    eqbench,\n    use_emotion=True,\n    use_strategy=False,\n    max_samples=MAX_SAMPLES\n)\n\neq_strategy = run_eqbench_with_heads(\n    dpo_model,\n    tokenizer,\n    eqbench,\n    use_emotion=False,\n    use_strategy=True,\n    max_samples=MAX_SAMPLES\n)\n\neq_full = run_eqbench_with_heads(\n    dpo_model,\n    tokenizer,\n    eqbench,\n    use_emotion=True,\n    use_strategy=True,\n    max_samples=MAX_SAMPLES\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:14:23.052910Z","iopub.execute_input":"2026-01-22T06:14:23.053246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = pd.DataFrame({\n    \"Configuration\": [\n        \"No Heads (Baseline)\",\n        \"Emotion Head Only\",\n        \"Strategy Head Only\",\n        \"Emotion + Strategy (Full)\"\n    ],\n    \"EQ-Bench Score\": [\n        eq_base,\n        eq_emotion,\n        eq_strategy,\n        eq_full\n    ]\n})\ndf2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.to_excel(\"results2.xlxs\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}